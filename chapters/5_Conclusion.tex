

This chapter concludes this dissertation, summarizing the former developed work, and giving up some topics that could be
interesting for future work.

From a general perspective, this project development contributed to solving the principal problem verified in Par-gem5, which is the 
quantum definition to get the best tradeoff between performance and accuracy. Further, it provided a flexible and well-functioning framework 
that enables the interaction between different tools. With these advancements, it is no longer required to study and understand 
the target system and benchmarks in detail, and Gem5 can now be utilized in co-simulation environments more frequently.
For this reason, the dissertation goal was accomplished. 

\section{Developed Work}

Concerning the dynamic quantum development, there were designed and tested four different algorithms. 
The ADALINE-based algorithm demonstrated the capability to adapt the quantum without any prior information and proved to be lightweight;
The Step Ladder usage enabled a higher performance gain in the majority of the cases, decreasing host time; 
\gls{ifp} allowed a deeper analysis of the simulation flow, highlighting potential regions where accuracy could be affected; And the Loop Detection 
algorithm provided loop identification while executing the benchmark, creating the possibility of adapting the quantum 
for the best value. 
After gathering all the results, the final decision was to incorporate all algorithms, with the exclusion of the repetition algorithm.
Due to its overhead in the simulation performance and weak accuracy gain, it was excluded from the final solution. However, it was shown 
that it can be an option when the workload has lots of repetitive tasks, enabling specific optimizations.

%talk about the simulation results of the final algorithm
Several tests were performed to evaluate it. In the end, it can be concluded the use of the dynamic version for the quantum choice brings 
greater benefits. When the NPB BT, LU, and SP with 32 and 64 simulated cores are not considered, there are gains in performance almost 
reaching 10\%, only sacrificing 0.5\% of accuracy. As mentioned on \autoref{cap:BM}, the previous tests have a particular characteristic, 
which is they execute nonlinear partial differential equations. This workload does not require inter-process communications thus, 
quantum can be increased without losing significant accuracy. The dynamic algorithm was designed to make it harder to increase the 
quantum value as the number of cores increases, due to the conclusions obtained in the Par-gem5 \cite*{pargem5} work. For this reason, 
this type of benchmark obtains a meaningful drop in performance when more than 16 cores are being simulated. However, the 
algorithm's design must be flexible, in a way that all benchmarks can have the best performance within accuracy limits.

Negative inaccuracy may continue to be present, as a consequence of Par-gem5 use. The dynamic quantum development was not able to solve this
issue, making necessary a deeper analysis of Par-gem5 design in order to find the problem. 

In conclusion, the better algorithm depends on the a-priori available information. If there is no information available, the dynamic version should be 
used because, even if the performance might be lower than it could be, it is guaranteed that accuracy will be above 95\%. On the other side,
knowing the details of the benchmark enables us to calculate the optimal quantum before initiating the simulation. It is important to 
remember that when increasing the quantum, while performance as a function of the quantum behaves similarly to a sigmoidal, inaccuracy 
grows linearly \cite*{BeyondQuantumTDSim}. If perfect accuracy is mandatory, the sequential mode should be selected since the usage of Par-gem5 
implies an accuracy cost. Also, simulations with a single simulated core should always be 
executed with the later mode. It was concluded that parallelization with only one simulated core results in a loss of accuracy 
without a significant gain in performance.

The developed co-simulation interface provided a new work environment between two different tools. Gem5 and SystemC were chosen to validate 
this, and the result was positive. Data integrity, data exchange, and synchronization between the tools were maintained during all the simulations. 
As a case study to stimulate this cooperation, the \gls{crc} device was chosen, 
passing all the requirements successfully. In addition, as part of the case study, 
memory integrity tests were designed and performed, where either for the success or the failure modeling, the expected results matched
with the real ones. On the other part, it was possible to verify that the dynamic quantum yielded improved results even with a significant 
time consumption in the communication between tools. Further, accuracy was always above 99\%, enchanting its performance. 

It is important to mention that this work regarding co-simulation between Gem5 and other tools is very poor, only existing a few works in the 
literature. Gem5 already has a defined co-simulation environment with SystemC in its official repository nevertheless, it does not 
fully adhere to the previously mentioned co-simulation definition because the two simulators are not running simultaneously in separate processes.
For this reason, this work contributes to the first steps in this subject. 


\section{Future Work}

Concerning future work, there are some aspects where improvements and new additions can be developed to have either an 
algorithm or a framework more robust and versatile.

The first aspect is the execution of more benchmarks. Although the algorithm was tested with a set of different and distinct benchmarks, 
more tests are required to have more performance results. These may reveal points where it can be improved, either with the development
of a new support algorithm or with a better tunning of the available parameters. PARSEC, SPEC2017, and STREAM are some examples of 
benchmarks where experiments can be conducted. 

The second aspect is the adaptation for the timing \gls{cpu} model. A future work of Par-gem5 \cite{pargem5} is to extend this solution 
to the timing mode thus, the developed approach should also function in this condition. Only the \gls{ifp} algorithm requires an 
adaptation, due to its \gls{pc} analysis method. The timing mode uses multiple events to model a transaction, which makes it difficult
to perform a correct examination of the executed instructions. In-order, and \gls{o3} demand an adaptation as well however, until 
the present date, no publications have been made on this matter regarding Par-gem5.

Another point is the usage of other targets. The developed algorithm was designed to tackle any target board nevertheless, as 
aforementioned, only VExpress\_gem5 was used for all the tests. Different boards with different characteristics may highlight flaws
that could not be spotted with the present one. Such usage may require its design and development in the platform, 
with the creation of all the necessary devices and connections. 

Finally, improvements in the co-simulation environment can be implemented. The main problem observed was the time consumption in the 
communication between tools. From a host time perspective, the workload on the memory comparison task corresponds to 87 \% of the 
whole simulation, where 82.2\% of this value, on average, corresponds to the waiting time for the payload response. As part of the solution
to speed up co-simulation, other \glspl*{ipc} techniques can be used, like shared memory. Although it is the 
fastest \gls{ipc} available, it is the unsafest one due to the lack of permission verifications hence, for this reason, a careful 
design must be employed. Nevertheless, it might not be enough to solve the problem, and other approaches must be considered.
